{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "klVnvb_efg3p",
        "nfshW7ISf5tn",
        "V74ebWnugIb0"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Selection By Filter Method**"
      ],
      "metadata": {
        "id": "aU4kwLxch0ww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's consider a dataset containing various features related to houses, such as size, number of bedrooms, number of bathrooms, location, and age. We want to predict the price of the houses based on these features. However, not all features may be equally relevant for predicting house prices. We can use feature selection techniques to identify the most important features for our prediction task."
      ],
      "metadata": {
        "id": "kHM-DETXh7pR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading Data\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset (example)\n",
        "data = {\n",
        "    'size': [1000, 1500, 1200, 1800],\n",
        "    'bedrooms': [2, 3, 2, 4],\n",
        "    'bathrooms': [1, 2, 1, 2],\n",
        "    'location': ['A', 'B', 'A', 'C'],\n",
        "    'age': [10, 5, 8, 15],\n",
        "    'price': [200000, 300000, 250000, 350000]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "X = df.drop(columns=['price'])  # Features\n",
        "y = df['price']  # Target variable\n",
        "y=pd.to_numeric(y)"
      ],
      "metadata": {
        "id": "riziKR3ZhveX"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ey3-VKHDjQGI",
        "outputId": "c225753e-314c-4302-c26c-90bc669d473d"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    200000.0\n",
            "1    300000.0\n",
            "2    250000.0\n",
            "3    350000.0\n",
            "Name: price, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use various feature selection techniques to identify the most relevant features. One common technique is to calculate feature importance scores using a machine learning model such as a decision tree-based model."
      ],
      "metadata": {
        "id": "RA72O2eEiRT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "# Define the preprocessing pipeline\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('onehot', OneHotEncoder(), ['location'])  # Apply one-hot encoding to 'location'\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")"
      ],
      "metadata": {
        "id": "PNamuSfDj8pE"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the feature importances, we can select the top k most important features."
      ],
      "metadata": {
        "id": "FXxQBzf_imts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the RandomForestRegressor model\n",
        "model = RandomForestRegressor()\n",
        "\n",
        "# Define the pipeline\n",
        "pipeline = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('model', model)\n",
        "])\n",
        "\n",
        "# Train the model\n",
        "pipeline.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "feature_importances = pipeline.named_steps['model'].feature_importances_\n",
        "\n",
        "k = min(k, len(X.columns))\n",
        "top_features_indices = feature_importances.argsort()[-k:][::-1]\n",
        "selected_features = X.columns[top_features_indices]\n",
        "print(\"Selected features:\", selected_features)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gOWAMNEPkfR5",
        "outputId": "1dec3038-a8df-418d-ee73-caf3c648fad4"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected features: Index(['location', 'age', 'size'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(k)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmym738ulPmZ",
        "outputId": "6ed236cb-b71c-4f4e-9782-c4a964a9bbad"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3\n",
            "[3 6 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Extraction using PCA**"
      ],
      "metadata": {
        "id": "FMV9FtZTu_kL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "\n",
        "# Generate some example data\n",
        "X = np.random.rand(100, 10)  # 100 samples, 10 features\n",
        "\n",
        "# Apply PCA for dimensionality reduction\n",
        "pca = PCA(n_components=3)  # Reduce to 3 principal components\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "print(\"Original data shape:\", X.shape)\n",
        "print(\"Transformed data shape (after PCA):\", X_pca.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srdt01ZemOV1",
        "outputId": "55e9c02d-55e2-4bec-cb28-091081f02cf6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original data shape: (100, 10)\n",
            "Transformed data shape (after PCA): (100, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dimensionality**\n"
      ],
      "metadata": {
        "id": "klVnvb_efg3p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensionality refers to the number of features or variables that are used to represent each data point in a dataset. In other words, it measures the number of dimensions or axes needed to describe a dataset's feature space.\n",
        "\n",
        "For example:\n",
        "\n",
        "1. In a dataset containing only one feature (e.g., height), each data point would be represented as a single point along a one-dimensional axis.\n",
        "\n",
        "2. In a dataset containing two features (e.g., height and weight), each data point would be represented as a point in a two-dimensional space, with one axis representing height and the other representing weight.\n",
        "\n",
        "3. In a dataset containing three features (e.g., height, weight, and age), each data point would be represented as a point in a three-dimensional space, with one axis representing height, another representing weight, and the third representing age.\n",
        "\n",
        "In general, as the number of features or dimensions increases, the complexity of the dataset's representation grows, and the computational and analytical challenges associated with working with the data also increase. This is known as the curse of dimensionality. Dimensionality reduction techniques are often employed to address these challenges and extract the most relevant information from high-dimensional datasets."
      ],
      "metadata": {
        "id": "-iNLsNpTgi8J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **curse of dimensionality**\n"
      ],
      "metadata": {
        "id": "nfshW7ISf5tn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The \"curse of dimensionality\" refers to various challenges and issues that arise when dealing with high-dimensional data in machine learning and data analysis. As the number of dimensions (features or variables) in a dataset increases, the amount of data required to adequately cover the space grows exponentially. This can lead to various problems such as increased computational complexity, sparsity of data points, and difficulty in visualization and interpretation.\n",
        "\n",
        "Here are some key aspects of the curse of dimensionality:\n",
        "\n",
        "Increased Computational Complexity:\n",
        "\n",
        "Algorithms operating in high-dimensional spaces often require more computational resources and time for processing and analysis.\n",
        "As the number of dimensions increases, the complexity of computations, such as distance calculations, grows exponentially.\n",
        "Sparsity of Data:\n",
        "\n",
        "In high-dimensional spaces, data points become increasingly sparse, meaning that the available data is spread thinly across the feature space.\n",
        "Sparse data can lead to difficulties in accurately estimating statistical properties, making it challenging to generalize from the data.\n",
        "Difficulty in Visualization and Interpretation:\n",
        "\n",
        "It becomes increasingly difficult to visualize and interpret data in high-dimensional spaces beyond three dimensions.\n",
        "Traditional visualization techniques, such as scatter plots and heatmaps, are limited in their effectiveness for high-dimensional data.\n",
        "Overfitting:\n",
        "\n",
        "High-dimensional spaces increase the risk of overfitting, where a model captures noise or irrelevant patterns in the data rather than the underlying structure.\n",
        "Overfitting can lead to poor generalization performance when applying the model to new, unseen data.\n",
        "To address the challenges posed by high-dimensional data, dimensionality reduction techniques are often employed."
      ],
      "metadata": {
        "id": "XjTjvccrgobM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Dimensionality reduction**\n"
      ],
      "metadata": {
        "id": "V74ebWnugIb0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensionality reduction refers to the process of reducing the number of input variables or features in a dataset while preserving the most relevant information.\n",
        "\n",
        "There are two main approaches to dimensionality reduction in deep learning:\n",
        "\n",
        "**Feature Selection:**\n",
        "\n",
        "Feature selection methods aim to identify a subset of the most informative features from the original feature set.\n",
        "Common techniques include filter methods (e.g., correlation-based feature selection), wrapper methods (e.g., recursive feature elimination), and embedded methods (e.g., L1 regularization).\n",
        "\n",
        "\n",
        "**Feature Extraction:**\n",
        "\n",
        "Feature extraction methods transform the original high-dimensional data into a lower-dimensional representation using techniques such as Principal Component Analysis (PCA), t-distributed Stochastic Neighbor Embedding (t-SNE), and autoencoders.\n",
        "These methods learn new representations of the data that capture the most relevant information while reducing dimensionality.\n",
        "By reducing the dimensionality of the data, dimensionality reduction techniques can alleviate the curse of dimensionality, improve computational efficiency, enhance model interpretability, and mitigate overfitting in deep learning models. However, it is essential to carefully select and apply dimensionality reduction methods based on the characteristics of the data and the specific requirements of the task at hand."
      ],
      "metadata": {
        "id": "nrrvuYMggt-V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature selection**"
      ],
      "metadata": {
        "id": "wQOYeF0ChPSy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature selection is the process of selecting a subset of relevant features (variables or attributes) from the original set of features in a dataset. This subset of features is chosen based on certain criteria to improve model performance, reduce computational complexity, and enhance interpretability. Feature selection is particularly important when dealing with high-dimensional datasets to mitigate the curse of dimensionality and avoid overfitting.\n",
        "\n",
        "There are three general classes of feature selection algorithms: Filter methods, wrapper methods and embedded methods."
      ],
      "metadata": {
        "id": "jqdJeD-VhNhw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature extraction**"
      ],
      "metadata": {
        "id": "v-oEl5WfpK4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature extraction is the process of transforming raw data into a set of features (representations) that are more suitable for modeling and analysis. In the context of machine learning, feature extraction aims to capture the most relevant information from the original data while reducing its dimensionality or complexity.\n",
        "\n",
        "Here's an overview of the feature extraction process:\n",
        "\n",
        "Data Representation:\n",
        "\n",
        "Feature extraction begins with the selection or creation of an appropriate representation of the raw data.\n",
        "This representation could be raw data itself (e.g., pixel values in images, word frequencies in text) or preprocessed data (e.g., normalized values, tokenized text).\n",
        "Feature Engineering:\n",
        "\n",
        "Feature engineering involves creating new features or transforming existing features to capture relevant information.\n",
        "This may include techniques such as scaling, normalization, binarization, one-hot encoding, or creating new features through mathematical operations.\n",
        "Dimensionality Reduction:\n",
        "\n",
        "Dimensionality reduction techniques are often applied to reduce the number of features while preserving the most important information.\n",
        "Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), t-distributed Stochastic Neighbor Embedding (t-SNE), and autoencoders are common dimensionality reduction methods used in feature extraction.\n",
        "Feature Selection:\n",
        "\n",
        "Feature selection aims to select a subset of the most relevant features from the original set of features.\n",
        "This can be done using various criteria such as statistical tests, feature importance scores, or domain knowledge.\n",
        "Model-Specific Feature Extraction:\n",
        "\n",
        "Some machine learning models, particularly deep learning models, may involve specific feature extraction techniques tailored to the model architecture.\n",
        "For example, convolutional neural networks (CNNs) automatically extract hierarchical features from images, while recurrent neural networks (RNNs) may learn sequential features from text data.\n",
        "Feature extraction is crucial for improving the performance, interpretability, and efficiency of machine learning models. It helps in reducing the dimensionality of the data, mitigating the curse of dimensionality, removing irrelevant features, and focusing on the most informative aspects of the data."
      ],
      "metadata": {
        "id": "u4uJ2rZmpH44"
      }
    }
  ]
}